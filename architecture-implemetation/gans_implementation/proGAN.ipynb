{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDygu/weu1x6HTpAtQHuZt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Discriminator & Generator"],"metadata":{"id":"5Sam9ZIs85hB"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn \n","import torch.nn.functional as F\n","from math import log2"],"metadata":{"id":"nQ6CT5TQ9BxT","executionInfo":{"status":"ok","timestamp":1671793328239,"user_tz":-330,"elapsed":1372,"user":{"displayName":"Samuel Davis","userId":"11210077990113460538"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["factors = [1, 1, 1, 1, 1/2, 1/4, 1/8, 1/16, 1/32]\n","\n","class WSConv2d(nn.Module): # Equalizer learning rate for Conv layer\n","  def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, gain=2):\n","    super().__init__()\n","    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","    self.scale = (gain / (in_channels * kernel_size**2)) ** 0.5\n","    # We dont want to scale the bias\n","    self.bias = self.conv.bias\n","    self.conv.bias = None\n","\n","    # initialize conv layer\n","    nn.init.normal_(self.conv.weight)\n","    nn.init.zeros_(self.bias)\n","\n","  def forward(self, x):\n","    return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n","\n","class PixelNorm(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.epsilon = 1e-8\n","\n","  def forward(self, x):\n","    return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n","\n","\n","class ConvBlock(nn.Module):\n","  def __init__(self, in_channels, out_channels, use_pixelnorm=True):\n","    super().__init__()\n","    self.conv1 = WSConv2d(in_channels, out_channels)\n","    self.conv2 = WSConv2d(out_channels, out_channels)\n","    self.leaky = nn.LeakyReLU(0.2)\n","    self.pn = PixelNorm()\n","    self.use_pn = use_pixelnorm\n","\n","  def forward(self, x):\n","    x = self.leaky(self.conv1(x))\n","    x = self.pn(x) if self.use_pn else x\n","    x = self.leaky(self.conv2(x))\n","    x = self.pn(x) if self.use_pn else x\n","    return x\n","\n","class Generator(nn.Module):\n","  def __init__(self, z_dim, in_channels, img_channels=3):\n","    super().__init__()\n","    self.initial = nn.Sequential(\n","        PixelNorm(),\n","        nn.ConvTranspose2d(z_dim, in_channels, 4, 1, 0), # 1x1 -> 4x4\n","        nn.LeakyReLU(0.2),\n","        WSConv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n","        nn.LeakyReLU(0.2),\n","        PixelNorm(),\n","    )\n","\n","    self.initial_rgb = WSConv2d(in_channels, img_channels, kernel_size=1, stride=1, padding=0)\n","    self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList([self.initial_rgb])\n","\n","    for i in range(len(factors) -1): # factor[i] -> factor[i+1]\n","      conv_in_c = int(in_channels * factors[i])\n","      conv_out_c = int(in_channels * factors[i+1])\n","      self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c))\n","      self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size=1, stride=1, padding=0))\n","\n","  def fade_in(self, alpha, upscaled, generated):\n","    return torch.tanh((1 - alpha)*upscaled + alpha*generated)\n","  \n","  def forward(self, x, alpha, steps):\n","    # steps=0 (4x4), steps=1 (8x8),....\n","    out = self.initial(x)\n","\n","    if steps == 0:\n","      return self.initial_rgb(out)\n","    \n","    for step in range(steps):\n","      upscaled = F.interpolate(out, scale_factor=2, mode='nearest')\n","      out = self.prog_blocks[step](upscaled)\n","    \n","    final_upscaled = self.rgb_layers[steps - 1](upscaled)\n","    final_out = self.rgb_layers[steps](out)\n","    return self.fade_in(alpha, final_upscaled, final_out)\n","\n","class Discriminator(nn.Module):\n","  def __init__(self, z_dim, in_channels, img_channels=3):\n","    super().__init__()\n","    self.prog_blocks, self.rgb_layers = nn.ModuleList(), nn.ModuleList()\n","    self.leaky = nn.LeakyReLU(0.2)\n","\n","    for i in range(len(factors) -1, 0, -1):\n","      conv_in_c = int(in_channels * factors[i])\n","      conv_out_c = int(in_channels * factors[i-1])\n","      self.prog_blocks.append(ConvBlock(conv_in_c, conv_out_c, use_pixelnorm=False))\n","      self.rgb_layers.append(WSConv2d(img_channels, conv_in_c, kernel_size=1, stride=1, padding=0))\n","\n","    # for 4x4 image resolution\n","    self.initial_rgb = WSConv2d(img_channels, in_channels, kernel_size=1, stride=1, padding=0)\n","    self.rgb_layers.append(self.initial_rgb)\n","    self.avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","    # block for 4x4 resolution\n","    self.final_block = nn.Sequential(\n","        WSConv2d(in_channels+1, in_channels, kernel_size=3, stride=1, padding=1),\n","        nn.LeakyReLU(0.2),\n","        WSConv2d(in_channels, in_channels, kernel_size=4, stride=1, padding=0),\n","        nn.LeakyReLU(0.2),\n","        WSConv2d(in_channels, 1, kernel_size=1, padding=0, stride=1),\n","    )\n","    \n","  \n","  def fade_in(self, alpha, downscaled, out):\n","    return alpha * out + (1 - alpha) * downscaled\n","\n","  def minibatch_std(self, x):\n","    batch_statistics = torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3]) # N x C x H x W -> N\n","    return torch.cat([x, batch_statistics], dim=1) # 512 -> 513\n","\n","  def forward(self, x, alpha, steps):\n","    # steps=0 (4x4), steps=1 (8x8),....\n","    cur_steps = len(self.prog_blocks) - steps\n","    out = self.leaky(self.rgb_layers[cur_steps](x))\n","\n","    if steps == 0:\n","      out = self.minibatch_std(out)\n","      return self.final_block(out).view(out.shape[0], -1)\n","\n","    downscaled = self.leaky(self.rgb_layers[cur_steps+1](self.avg_pool(x)))\n","    out = self.avg_pool(self.prog_blocks[cur_steps](out))\n","    out = self.fade_in(alpha, downscaled, out)\n","    \n","    for step in range(cur_steps + 1, len(self.prog_blocks)):\n","      out = self.prog_blocks[step](out)\n","      out = self.avg_pool(out)\n","\n","    out = self.minibatch_std(out)\n","    return self.final_block(out).view(out.shape[0], -1)\n","\n","# if __name__ == \"__main__\":\n","#     Z_DIM = 50\n","#     IN_CHANNELS = 256\n","#     gen = Generator(Z_DIM, IN_CHANNELS, img_channels=3)\n","#     critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=3)\n","\n","#     for img_size in [4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n","#         num_steps = int(log2(img_size / 4))\n","#         x = torch.randn((1, Z_DIM, 1, 1))\n","#         z = gen(x, 0.5, steps=num_steps)\n","#         assert z.shape == (1, 3, img_size, img_size)\n","#         out = critic(z, alpha=0.5, steps=num_steps)\n","#         assert out.shape == (1, 1)\n","#         print(f\"Success! At img size: {img_size}\")"],"metadata":{"id":"VfqckBxZ02Pg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Config "],"metadata":{"id":"VtKXQihhszGo"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"NGkGqx1D82FU","executionInfo":{"status":"ok","timestamp":1671794476198,"user_tz":-330,"elapsed":544,"user":{"displayName":"Samuel Davis","userId":"11210077990113460538"}}},"outputs":[],"source":["import cv2\n","import torch\n","from math import log2\n","\n","START_TRAIN_AT_IMG_SIZE = 4\n","DATASET = 'celeb_dataset'\n","CHECKPOINT_GEN = \"generator.pth\"\n","CHECKPOINT_CRITIC = \"critic.pth\"\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","SAVE_MODEL = True\n","LOAD_MODEL = False\n","LEARNING_RATE = 1e-3\n","BATCH_SIZES = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n","CHANNELS_IMG = 3\n","Z_DIM = 256  # should be 512 in original paper\n","IN_CHANNELS = 256  # should be 512 in original paper\n","CRITIC_ITERATIONS = 1\n","LAMBDA_GP = 10\n","PROGRESSIVE_EPOCHS = [20] * len(BATCH_SIZES)\n","FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n","NUM_WORKERS = 4"]},{"cell_type":"markdown","source":["# Utils"],"metadata":{"id":"rdnDaBfat5Xu"}},{"cell_type":"code","source":["import torch\n","import random\n","import numpy as np\n","import os\n","import torchvision\n","import torch.nn as nn\n","from torchvision.utils import save_image\n","from scipy.stats import truncnorm\n","\n","# Print losses occasionally and print to tensorboard\n","def plot_to_tensorboard(\n","    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n","):\n","    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n","\n","    with torch.no_grad():\n","        # take out (up to) 8 examples to plot\n","        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n","        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n","        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n","        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n","\n","\n","def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n","    BATCH_SIZE, C, H, W = real.shape\n","    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * beta + fake.detach() * (1 - beta)\n","    interpolated_images.requires_grad_(True)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images, alpha, train_step)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty\n","\n","\n","def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    checkpoint = {\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","\n","    # If we don't do this then it will just have learning rate of old checkpoint\n","    # and it will lead to many hours of debugging \\:\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","def seed_everything(seed=42):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","def generate_examples(gen, steps, truncation=0.7, n=100):\n","    \"\"\"\n","    Tried using truncation trick here but not sure it actually helped anything, you can\n","    remove it if you like and just sample from torch.randn\n","    \"\"\"\n","    gen.eval()\n","    alpha = 1.0\n","    for i in range(n):\n","        with torch.no_grad():\n","            noise = torch.tensor(truncnorm.rvs(-truncation, truncation, size=(1, Z_DIM, 1, 1)), device=DEVICE, dtype=torch.float32)\n","            img = gen(noise, alpha, steps)\n","            save_image(img*0.5+0.5, f\"saved_examples/img_{i}.png\")\n","    gen.train()"],"metadata":{"id":"N9jL8J45t6ol","executionInfo":{"status":"ok","timestamp":1671794561750,"user_tz":-330,"elapsed":1535,"user":{"displayName":"Samuel Davis","userId":"11210077990113460538"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"ChKEBEXm0U6d"}},{"cell_type":"code","source":["!pip install -q kaggle\n","!mkdir ~/.kaggle\n","!cp /content/kaggle.json ~/.kaggle\n","!chmod 600 ~/.kaggle/kaggle.json\n","!kaggle datasets download -d lamsimon/celebahq\n","!unzip /content/celebahq.zip"],"metadata":{"id":"--_DN4rR0WdS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"Ml24ymaouUMm"}},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from math import log2\n","from tqdm import tqdm"],"metadata":{"id":"O_6bGrzZuk4s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.backends.cudnn.benchmarks = True # performance benefits\n","\n","def get_loader(image_size):\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize((image_size, image_size)),\n","            transforms.ToTensor(),\n","            transforms.RandomHorizontalFlip(p=0.5),\n","            transforms.Normalize(\n","                [0.5 for _ in range(CHANNELS_IMG)],\n","                [0.5 for _ in range(CHANNELS_IMG)],\n","            ),\n","        ]\n","    )\n","    batch_size = BATCH_SIZES[int(log2(image_size / 4))]\n","    dataset = datasets.ImageFolder(root=DATASET, transform=transform)\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=NUM_WORKERS,\n","        pin_memory=True,\n","    )\n","    return loader, \n","\n","def train_fn(\n","    critic,\n","    gen,\n","    loader,\n","    dataset,\n","    step,\n","    alpha,\n","    opt_critic,\n","    opt_gen,\n","    tensorboard_step,\n","    writer,\n","    scaler_gen,\n","    scaler_critic,\n","):\n","    loop = tqdm(loader, leave=True)\n","    for batch_idx, (real, _) in enumerate(loop):\n","        real = real.to(DEVICE)\n","        cur_batch_size = real.shape[0]\n","\n","        # Train Critic: max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n","        # which is equivalent to minimizing the negative of the expression\n","        noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(DEVICE)\n","\n","        with torch.cuda.amp.autocast():\n","            fake = gen(noise, alpha, step)\n","            critic_real = critic(real, alpha, step)\n","            critic_fake = critic(fake.detach(), alpha, step)\n","            gp = gradient_penalty(critic, real, fake, alpha, step, device=DEVICE)\n","            loss_critic = (\n","                -(torch.mean(critic_real) - torch.mean(critic_fake))\n","                + config.LAMBDA_GP * gp\n","                + (0.001 * torch.mean(critic_real ** 2))\n","            )\n","\n","        opt_critic.zero_grad()\n","        scaler_critic.scale(loss_critic).backward()\n","        scaler_critic.step(opt_critic)\n","        scaler_critic.update()\n","\n","        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n","        with torch.cuda.amp.autocast():\n","            gen_fake = critic(fake, alpha, step)\n","            loss_gen = -torch.mean(gen_fake)\n","\n","        opt_gen.zero_grad()\n","        scaler_gen.scale(loss_gen).backward()\n","        scaler_gen.step(opt_gen)\n","        scaler_gen.update()\n","\n","        # Update alpha and ensure less than 1\n","        alpha += cur_batch_size / (\n","            (PROGRESSIVE_EPOCHS[step] * 0.5) * len(dataset)\n","        )\n","        alpha = min(alpha, 1)\n","\n","        if batch_idx % 500 == 0:\n","            with torch.no_grad():\n","                fixed_fakes = gen(config.FIXED_NOISE, alpha, step) * 0.5 + 0.5\n","            plot_to_tensorboard(\n","                writer,\n","                loss_critic.item(),\n","                loss_gen.item(),\n","                real.detach(),\n","                fixed_fakes.detach(),\n","                tensorboard_step,\n","            )\n","            tensorboard_step += 1\n","\n","        loop.set_postfix(\n","            gp=gp.item(),\n","            loss_critic=loss_critic.item(),\n","        )\n","\n","    return tensorboard_step, alpha\n","\n","def main():\n","  # initialize gen and disc, note: discriminator should be called critic,\n","  # according to WGAN paper (since it no longer outputs between [0, 1])\n","  # but really who cares..\n","  gen = Generator(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n","  critic = Discriminator(Z_DIM, IN_CHANNELS, img_channels=CHANNELS_IMG).to(DEVICE)\n","\n","  # initialize optimizers and scalers for FP16 training\n","  opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n","  opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.99))\n","  scaler_critic = torch.cuda.amp.GradScaler()\n","  scaler_gen = torch.cuda.amp.GradScaler()\n","\n","  # for tensorboard plotting\n","  writer = SummaryWriter(f\"logs/gan1\")\n","\n","  if config.LOAD_MODEL:\n","      load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n","      load_checkpoint(CHECKPOINT_CRITIC, critic, opt_critic, LEARNING_RATE)\n","\n","  gen.train()\n","  critic.train()\n","  tensorboard_step = 0\n","  step = int(log2(START_TRAIN_AT_IMG_SIZE / 4))\n","\n","  for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n","    alpha = 1e-5\n","    loader, dataset = get_loader(4*2**step)\n","    print(f'Image size: {4*2**step}')\n","    for epoch in range(num_epochs):\n","      print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","      tensorboard_step, alpha = train_fn(\n","          critic,\n","          gen,\n","          loader,\n","          dataset,\n","          step,\n","          alpha,\n","          opt_critic,\n","          opt_gen,\n","          tensorboard_step,\n","          writer,\n","          scaler_gen,\n","          scaler_critic,\n","      )\n","      if SAVE_MODEL:\n","          save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n","          save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n","    \n","    step += 1\n","\n","\n","if __name == '__main__':\n","  main()"],"metadata":{"id":"hYQUIvbOLYJ1"},"execution_count":null,"outputs":[]}]}