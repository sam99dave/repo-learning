{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJWEVzJRQfl9h4fR/GtV1M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"rbu590u5KQag"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","class ConvBlock(nn.Module):\n","  def __init__(self, in_channels, out_channels, use_act, **kwargs):\n","    super().__init__()\n","    self.cnn = nn.Conv2d(\n","        in_channels,\n","        out_channels,\n","        **kwargs,\n","        bias=True\n","    )\n","\n","    self.act = nn.LeakyReLU(0.2, inplace=True) if use_act else nn.Identity()\n","\n","  def forward(self, x):\n","    return self.cnn(x)\n","  \n","class UpsampleBlock(nn.Module):\n","  def __init__(self, in_c, scale_factor=2):\n","    super().__init__()\n","    self.upsample = nn.Upsample(scale_factor=scale_factor, mode='nearest')\n","    self.conv = nn.Conv2d(in_c, in_c, 3, 1, 1, bias=True)\n","    self.act = nn.LeakyReLU(0.2, inplace=True)\n","  \n","  def forward(self, x):\n","    return self.act(self.conv(self.upsample(x)))\n","\n","class DenseResidualBlock(nn.Module):\n","  def __init__(self, in_channels, channels=32, residual_beta=0.2):\n","    super().__init__()\n","    self.residual_beta = residual_beta\n","    self.blocks = nn.ModuleList()\n","    for i in range(5):\n","      self.blocks.append(\n","          ConvBlock(\n","              in_channels + channels * i,\n","              channels if i <= 3 else in_channels,\n","              kernel_size=3,\n","              stride=1,\n","              padding=1,\n","              use_act=True if i <= 3 else False,\n","          )\n","      )\n","    \n","  def forward(self, x):\n","    new_inputs = x\n","    for block in self.blocks:\n","      out = block(new_inputs)\n","      new_inputs = torch.cat([new_inputs, out], dim=1)\n","    \n","    return self.residual_beta * out + x\n","  \n","class RRDB(nn.Module):\n","  def __init__(self, in_channels, residual_beta=0.2):\n","    super().__init__()\n","    self.residual_beta = residual_beta\n","    self.rrdb = nn.Sequential(*[DenseResidualBlock(in_channels) for _ in range(3)])\n","\n","  def forward(self, x):\n","    return self.residual_beta*self.rrdb(x) + x\n","  \n","class Generator(nn.Module):\n","  def __init__(self, in_channels=3, num_channels=64, num_blocks=23):\n","    super().__init__()\n","    self.initial = nn.Conv2d(\n","        in_channels,\n","        num_channels,\n","        kernel_size=3,\n","        stride=1,\n","        padding=1,\n","        bias=True,\n","    )\n","\n","    self.residuals = nn.Sequential(\n","        *[RRDB(num_channels) for _ in range(num_blocks)]\n","    )\n","\n","    self.conv = nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1)\n","    self.upsampler = nn.Sequential(\n","        UpsampleBlock(num_channels),\n","        UpsampleBlock(num_channels),\n","    )\n","\n","    self.final = nn.Sequential(\n","        nn.Conv2d(num_channels, num_channels, 3, 1, 1, bias=True),\n","        nn.LeakyReLU(0.2, inplace=True),\n","        nn.Conv2d(num_channels, in_channels, 3, 1, 1, bias=True),\n","    )\n","\n","  def forward(self, x):\n","    initial = self.initial(x)\n","    x = self.conv(self.residuals(initial)) + initial\n","    x = self.final(self.upsampler(x))\n","\n","    return x\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, in_channels=3, features=[64, 64, 128, 128, 256, 256, 512, 512]):\n","        super().__init__()\n","        blocks = []\n","        for idx, feature in enumerate(features):\n","            blocks.append(\n","                ConvBlock(\n","                    in_channels,\n","                    feature,\n","                    kernel_size=3,\n","                    stride=1 + idx % 2,\n","                    padding=1,\n","                    use_act=True,\n","                ),\n","            )\n","            in_channels = feature\n","\n","        self.blocks = nn.Sequential(*blocks)\n","        self.classifier = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((6, 6)),\n","            nn.Flatten(),\n","            nn.Linear(512 * 6 * 6, 1024),\n","            nn.LeakyReLU(0.2, inplace=True),\n","            nn.Linear(1024, 1),\n","        )\n","\n","    def forward(self, x):\n","        x = self.blocks(x)\n","        return self.classifier(x)\n","\n","def initialize_weights(model, scale=0.1):\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d):\n","            nn.init.kaiming_normal_(m.weight.data)\n","            m.weight.data *= scale\n","\n","        elif isinstance(m, nn.Linear):\n","            nn.init.kaiming_normal_(m.weight.data)\n","            m.weight.data *= scale\n","\n","\n","# def test():\n","#     gen = Generator()\n","#     disc = Discriminator()\n","#     low_res = 24\n","#     x = torch.randn((5, 3, low_res, low_res))\n","#     gen_out = gen(x)\n","#     disc_out = disc(gen_out)\n","\n","#     print(gen_out.shape)\n","#     print(disc_out.shape)\n","\n","# if __name__ == \"__main__\":\n","#     test()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBDl6EPrKSZf","executionInfo":{"status":"ok","timestamp":1673499420013,"user_tz":-330,"elapsed":2247,"user":{"displayName":"Samuel Davis","userId":"11210077990113460538"}},"outputId":"0a00eb46-68dd-4c35-d7f8-812b6a61d5be"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 3, 96, 96])\n","torch.Size([5, 1])\n"]}]},{"cell_type":"markdown","source":["# Others"],"metadata":{"id":"rJoZ7glEKTKd"}},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"SE-v3wsQV5KA"}},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","LOAD_MODEL = True\n","SAVE_MODEL = True\n","CHECKPOINT_GEN = \"gen.pth\"\n","CHECKPOINT_DISC = \"disc.pth\"\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","LEARNING_RATE = 1e-4\n","NUM_EPOCHS = 10000\n","BATCH_SIZE = 16\n","LAMBDA_GP = 10\n","NUM_WORKERS = 4\n","HIGH_RES = 128\n","LOW_RES = HIGH_RES // 4\n","IMG_CHANNELS = 3\n","\n","highres_transform = A.Compose(\n","    [\n","        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n","        ToTensorV2(),\n","    ]\n",")\n","\n","lowres_transform = A.Compose(\n","    [\n","        A.Resize(width=LOW_RES, height=LOW_RES, interpolation=Image.BICUBIC),\n","        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n","        ToTensorV2(),\n","    ]\n",")\n","\n","both_transforms = A.Compose(\n","    [\n","        A.RandomCrop(width=HIGH_RES, height=HIGH_RES),\n","        A.HorizontalFlip(p=0.5),\n","        A.RandomRotate90(p=0.5),\n","    ]\n",")\n","\n","test_transform = A.Compose(\n","    [\n","        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1]),\n","        ToTensorV2(),\n","    ]\n",")"],"metadata":{"id":"KmL8bB2uV5Z9","executionInfo":{"status":"ok","timestamp":1673499513586,"user_tz":-330,"elapsed":3677,"user":{"displayName":"Samuel Davis","userId":"11210077990113460538"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Utils"],"metadata":{"id":"sAwWymgaKWsJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DG1AWrRJm1c"},"outputs":[],"source":["import torch\n","import os\n","import numpy as np\n","from PIL import Image\n","from torchvision.utils import save_image\n","\n","\n","def gradient_penalty(critic, real, fake, device):\n","    BATCH_SIZE, C, H, W = real.shape\n","    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n","    interpolated_images = real * alpha + fake.detach() * (1 - alpha)\n","    interpolated_images.requires_grad_(True)\n","\n","    # Calculate critic scores\n","    mixed_scores = critic(interpolated_images)\n","\n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=interpolated_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores),\n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    gradient = gradient.view(gradient.shape[0], -1)\n","    gradient_norm = gradient.norm(2, dim=1)\n","    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n","    return gradient_penalty\n","\n","\n","def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"=> Saving checkpoint\")\n","    checkpoint = {\n","        \"state_dict\": model.state_dict(),\n","        \"optimizer\": optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n","\n","\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    print(\"=> Loading checkpoint\")\n","    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n","    # model.load_state_dict(checkpoint)\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","\n","    # If we don't do this then it will just have learning rate of old checkpoint\n","    # and it will lead to many hours of debugging \\:\n","    for param_group in optimizer.param_groups:\n","        param_group[\"lr\"] = lr\n","\n","\n","def plot_examples(low_res_folder, gen):\n","    files = os.listdir(low_res_folder)\n","\n","    gen.eval()\n","    for file in files:\n","        image = Image.open(\"test_images/\" + file)\n","        with torch.no_grad():\n","            upscaled_img = gen(\n","                test_transform(image=np.asarray(image))[\"image\"]\n","                .unsqueeze(0)\n","                .to(DEVICE)\n","            )\n","        save_image(upscaled_img, f\"saved/{file}\")\n","    gen.train()"]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"MOmmX8OmKa1v"}},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import time\n","import torch.nn\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from PIL import Image\n","import cv2\n","\n","\n","class MyImageFolder(Dataset):\n","    def __init__(self, root_dir):\n","        super(MyImageFolder, self).__init__()\n","        self.data = []\n","        self.root_dir = root_dir\n","        self.class_names = os.listdir(root_dir)\n","\n","        for index, name in enumerate(self.class_names):\n","            files = os.listdir(os.path.join(root_dir, name))\n","            self.data += list(zip(files, [index] * len(files)))\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        img_file, label = self.data[index]\n","        root_and_dir = os.path.join(self.root_dir, self.class_names[label])\n","\n","        image = cv2.imread(os.path.join(root_and_dir, img_file))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        both_transform = both_transforms(image=image)[\"image\"]\n","        low_res = lowres_transform(image=both_transform)[\"image\"]\n","        high_res = highres_transform(image=both_transform)[\"image\"]\n","        return low_res, high_res\n","\n","\n","# def test():\n","#     dataset = MyImageFolder(root_dir=\"data/\")\n","#     loader = DataLoader(dataset, batch_size=8)\n","\n","#     for low_res, high_res in loader:\n","#         print(low_res.shape)\n","#         print(high_res.shape)\n","\n","\n","# if __name__ == \"__main__\":\n","#     test()"],"metadata":{"id":"slOr_iLjKjDD","executionInfo":{"status":"ok","timestamp":1673499570578,"user_tz":-330,"elapsed":3,"user":{"displayName":"Samuel Davis","userId":"11210077990113460538"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Loss"],"metadata":{"id":"E__yQMJfWS5C"}},{"cell_type":"code","source":["import torch.nn as nn\n","from torchvision.models import vgg19\n","\n","class VGGLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.vgg = vgg19(pretrained=True).features[:35].eval().to(DEVICE)\n","\n","        for param in self.vgg.parameters():\n","            param.requires_grad = False\n","\n","        self.loss = nn.MSELoss()\n","\n","    def forward(self, input, target):\n","        vgg_input_features = self.vgg(input)\n","        vgg_target_features = self.vgg(target)\n","        return self.loss(vgg_input_features, vgg_target_features)"],"metadata":{"id":"ZvjqTAhYWLbC","executionInfo":{"status":"ok","timestamp":1673499614860,"user_tz":-330,"elapsed":2,"user":{"displayName":"Samuel Davis","userId":"11210077990113460538"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"7XkiiNhgWeXV"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","\n","torch.backends.cudnn.benchmark = True\n","\n","def train_fn(\n","    loader,\n","    disc,\n","    gen,\n","    opt_gen,\n","    opt_disc,\n","    l1,\n","    vgg_loss,\n","    g_scaler,\n","    d_scaler,\n","    writer,\n","    tb_step,\n","):\n","    loop = tqdm(loader, leave=True)\n","\n","    for idx, (low_res, high_res) in enumerate(loop):\n","        high_res = high_res.to(DEVICE)\n","        low_res = low_res.to(DEVICE)\n","\n","        with torch.cuda.amp.autocast():\n","            fake = gen(low_res)\n","            critic_real = disc(high_res)\n","            critic_fake = disc(fake.detach())\n","            gp = gradient_penalty(disc, high_res, fake, device=DEVICE)\n","            loss_critic = (\n","                -(torch.mean(critic_real) - torch.mean(critic_fake))\n","                + LAMBDA_GP * gp\n","            )\n","\n","        opt_disc.zero_grad()\n","        d_scaler.scale(loss_critic).backward()\n","        d_scaler.step(opt_disc)\n","        d_scaler.update()\n","\n","        # Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n","        with torch.cuda.amp.autocast():\n","            l1_loss = 1e-2 * l1(fake, high_res)\n","            adversarial_loss = 5e-3 * -torch.mean(disc(fake))\n","            loss_for_vgg = vgg_loss(fake, high_res)\n","            gen_loss = l1_loss + loss_for_vgg + adversarial_loss\n","\n","        opt_gen.zero_grad()\n","        g_scaler.scale(gen_loss).backward()\n","        g_scaler.step(opt_gen)\n","        g_scaler.update()\n","\n","        writer.add_scalar(\"Critic loss\", loss_critic.item(), global_step=tb_step)\n","        tb_step += 1\n","\n","        if idx % 100 == 0 and idx > 0:\n","            plot_examples(\"test_images/\", gen)\n","\n","        loop.set_postfix(\n","            gp=gp.item(),\n","            critic=loss_critic.item(),\n","            l1=l1_loss.item(),\n","            vgg=loss_for_vgg.item(),\n","            adversarial=adversarial_loss.item(),\n","        )\n","\n","    return tb_step\n","\n","\n","def main():\n","    dataset = MyImageFolder(root_dir=\"data/\")\n","    loader = DataLoader(\n","        dataset,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        pin_memory=True,\n","        num_workers=NUM_WORKERS,\n","    )\n","    gen = Generator(in_channels=3).to(DEVICE)\n","    disc = Discriminator(in_channels=3).to(DEVICE)\n","    initialize_weights(gen)\n","    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n","    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n","    writer = SummaryWriter(\"logs\")\n","    tb_step = 0\n","    l1 = nn.L1Loss()\n","    gen.train()\n","    disc.train()\n","    vgg_loss = VGGLoss()\n","\n","    g_scaler = torch.cuda.amp.GradScaler()\n","    d_scaler = torch.cuda.amp.GradScaler()\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(\n","            CHECKPOINT_GEN,\n","            gen,\n","            opt_gen,\n","            LEARNING_RATE,\n","        )\n","        load_checkpoint(\n","            CHECKPOINT_DISC,\n","            disc,\n","            opt_disc,\n","            LEARNING_RATE,\n","        )\n","\n","\n","    for epoch in range(NUM_EPOCHS):\n","        tb_step = train_fn(\n","            loader,\n","            disc,\n","            gen,\n","            opt_gen,\n","            opt_disc,\n","            l1,\n","            vgg_loss,\n","            g_scaler,\n","            d_scaler,\n","            writer,\n","            tb_step,\n","        )\n","\n","        if SAVE_MODEL:\n","            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n","            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n","\n","\n","if __name__ == \"__main__\":\n","    try_model = True\n","\n","    if try_model:\n","        # Will just use pretrained weights and run on images\n","        # in test_images/ and save the ones to SR in saved/\n","        gen = Generator(in_channels=3).to(DEVICE)\n","        opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n","        load_checkpoint(\n","            CHECKPOINT_GEN,\n","            gen,\n","            opt_gen,\n","            LEARNING_RATE,\n","        )\n","        plot_examples(\"test_images/\", gen)\n","    else:\n","        # This will train from scratch\n","        main()"],"metadata":{"id":"ZrF0oj8BWZSF"},"execution_count":null,"outputs":[]}]}